---
title: 'Topic 1: Diving Right In '
output:
  html_document: default
  toc: TRUE
  toc_float: TRUE
---

#### We are going to jump right in with some data analysis today!  We'll work with some rainfall data from a meteorological station near Boston that has data going back to the 1890's.

#### When you open an **R notebook** (.Rmd) file in **RStudio** you will see the text, code and output all in your **Editor pane**.  When you **Preview** or **Knit** the file it will be nicely rendered as a **HTML** file (*other formats such as PDF are also available*) in your **Viewer pane**. 

<br/>

First we'll install the packages that we need to run our analysis.  Once you've installed the packages they are on your computer and don't need to be installed again.

Note that I've commented out the install function by using the # symbol.  I did this since, I've already installed this package on my computer.  You'll want to uncomment (remove the # symbol) so that you can run the install function `install.package("tidyverse")`
```{r Install Packages}
# install.package("tidyverse") 
```

First we'll load in the packages that we need to run our analysis.  When you create a new project you'll need to load in the packages that you want to use (however you will not need to reinstall them).  The `libary()` function is used to load packages.
```{r Load Packages}
library(tidyverse)
library(lubridate)
```

<br/>

Now we'll load in the data from the web and clean it up a bit
```{r Load in Data}
## Load in data
met_data <- read_csv('http://stahlm.github.io/ENS_215/Data/Blue_Hills_MA_Precip_Temp_Data_1.csv')


met_data <- mutate(met_data,DATE = parse_date_time(DATE,"mdy")) # convert date into computable format

met_data <- select(met_data,-c(TOBS)) # remove the TOBS column, since we won't use it
```

We've got the data stored on our computer in a nice format called a **data frame**.  Let's take a look at data and get an idea of what we're working with
```{r Preview the Data}
head(met_data) # shows us the first few rows and provides info about data types
tail(met_data) # shows us the last few rows and provides info about data types

```
We see that we've got info about the name of the meteorological station, the dates when measurements were taken, precipitation in inches (PRCP), and the daily min (TMIN) and max (TMAX) temperatures in degrees Fahrenheit.

<br/>

Now let's get some summary statistics for our data.  This will help us to get a quick overview of our dataset.
```{r}
summary(met_data)
```
We can already begin to make some interesting observations based on these quick summary statistics.   

* What was the hottest temperature record?  
* What was the coldest temp recorded?  
* Was there ever a day where the temperature never exceeded 0 degrees F?  
* Does it rain on most days?
* What else do you observe? 

<br/>

Let's make a quick plot of the data.
```{r}
ggplot(met_data) + geom_line(aes(x = DATE, y = PRCP),color = "blue") 
```

Now we've got a basic figure that is helpful for examining and understanding our data.  When you are exploring data it's useful to quickly plot it up to help you gain understanding and formulate questions/hypotheses.  We can create highly customized, publication figures in R by adding some additional code (we'll learn more about this during the term).

Did you learn anything new about the data by plotting it?

<br/>

#### Now let's create some new dataframes that help us gain further insight into the data
Looking at the daily data, it is difficult to see how total precipitation varies from year to year.  Let's create a new variable with annual precipitation data.
```{r}
met_data <- mutate(met_data,year_val = year(DATE), month_val = month(DATE), day_val = day(DATE)) # add columns for year, month, and day

annual_precip <- group_by(met_data,year_val) # group the data by year
annual_precip<- summarise(annual_precip,tot_precip = sum(PRCP,na.rm = TRUE)) # create a new data frame that has the total annual precipitation (inches)
```


Let' plot the data and see how annual precipitation varies between years

```{r}
ggplot(annual_precip) + geom_line(aes(x = year_val, y = tot_precip),color = "blue") + geom_point(aes(x = year_val, y = tot_precip),color = "black") 
```

What do you observe here?  Any interesting features of the annual precipitation data?


Let's check how much it rained last year
```{r}
filter(annual_precip,year_val == 2017)
```

Now let's summarize the data by month.  This way we can see if there are seasonal patterns in rainfall for the area around Boston and how a given month's rainfall varies between years.
```{r}

# Group the data by year and month
monthly_precip <- group_by(met_data,year_val,month_val) %>% summarise(tot_precip = sum(PRCP, na.rm = TRUE))

# Create a box plot of the monthly precipitation data
ggplot(monthly_precip,aes(x=factor(month_val),y=tot_precip)) + geom_boxplot()
```

Is there much variability in total rainfall between months?  How about within a given month?


How rainy was it in the year you were born?  (Hint: filter on the annual_precip data)
```{r}
# Your code here
```

How rainy was it in the month you were born? (Hint: use the filter function on the monthly_precip and filter by multiple criteria)  
```{r}
# Your code here
```


Did it rain on the day you were born? (Hint: use the filter function on the met_data and filter by multiple criteria)

```{r}
# Your code here
```

<br/>

##### In your first analysis you analyzed nearly 50,000 daily precipitation measurements!  You were able to summarize, visualize and filter the data in ways that would have been tedious, error-prone, and time-consuming if you have tried to do it in Excel.  You can also modify and re-run your analysis to answer new questions or work with a different dataset, which would require starting from scratch in Excel.  Furthermore, you've generated a nice notebook that clearly documents your code, results and interpretation all in one place!  You could easily share this notebook with colleagues when working on collaborative projects and you have a clearly documented and reproducible workflow.