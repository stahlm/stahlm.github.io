---
title: "R Notebook"
output: html_notebook
---


```{r}
library(tidyverse)
library(lubridate)
```

```{r warning= F, message= F}
library(leaflet)
library(sf)
library(tmap)
```


library(tidycensus)

library(dataRetrieval)

library(rnoaa)


In today's lecture we are going to learn how to make use of a number of R packages that allow you to directly query and access data from some notable environmental databases.  These data access packages are incredibly powerful as they allow you to streamline your workflow by combining the data identification and acquisition steps directly with your data analysis.  They also provide a highly efficient and reproducible approach to data identification and acquisition - allowing you to easily adapt or expand your research question with minimal additional effort.  

There are many packages that have been developed for accessing data through R, including a wide range of packages for accessing environmental datasets.  Today we will look at a few notable packages and in the coming weeks we will explore some additional ones.  The packages below highlight a number of notable research-quality datasets that are likely to be useful in many of your class projects, theses, and future research/work.    


## Climate data

In much of our environmental research we need to use climate/meteorological data.  While there are a host of organization/agencies that provide freely available research quality datasets, locating and accessing the data can often be time-consuming and difficult.  Furthermore, once you've located the data (generally through the website of the agency/organization collecting the data) you typically need to download the file (e.g., csv, Excel), load it into R, and reformat the data for your analysis.  The R packages below allow you to quickly locate the data of interest and directly load it into R in a format that is ready to use in R - typically all within a single function call!  


### GSODR package

The [**Global Surface Summary of the Day - GSOD**](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516) is a dataset from the National Oceanic and Atmospheric Administration (NOAA) that provides daily average values for a range of meteorological variables (e.g., air temperature, precipitation, wind speed, barometric pressure) at thousands of locations around the globe.  These daily average data are computed hourly observations at meteorological stations (i.e., they are actual observations and not model/simulation outputs) and the data goes back upwards of 90 years at some locations.  FYI, the hourly data is available through the Integrated Surface Hourly (ISH) dataset from NOAA, which we will learn how to access later in today's lecture.  

We will learn how to use the [`GSODR` package](https://docs.ropensci.org/GSODR/) to directly access this data.  

First let's load in the package (FYI, you will need to install the package first if you have not installed it before)

https://cran.r-project.org/web/packages/GSODR/vignettes/GSODR.html
```{r}
library(GSODR)
```



The package comes with a dataset that has all of the available meteorological stations and their location information/details (e.g., lat/long, country, state, period of data coverage).  We will load in this data below.  

```{r}
isd_history <- load(system.file("extdata", "isd_history.rda", package = "GSODR"))
```


Take a look at the `isd_history` data frame to familiarize yourself with what it contains.  You'll see that you could use this information to easily identify sites of interest to your particular research question.  Below you can see a map of all of the available meteorological stations in the GSOD database.  

```{r warning= F, message= F, echo = F}
library(leaflet)

map_gsod <- isd_history  %>% leaflet() %>% 
  addProviderTiles(providers$OpenStreetMap) %>% 
  addMarkers(~ LON, ~ LAT, popup = ~ STNID, clusterOptions = T)

map_gsod
```




If you identified a station(s) of interest you can download the data using the `get_GSOD()` function.  Let's try this out for Windhoek, which is the capital of the country of Namibia (located in SW Africa).  To download data for a given station, you need to supply the station ID (STNID) and the years of interest.  You can locate the STNID in the `isd_history` data frame.   

725090-14739
```{r}
met_df <- get_GSOD(years = 2018:2021, station = "681100-99999")
```


Take a look at the `met_df` data frame.  The description of the variables can [be found here](https://docs.ropensci.org/GSODR/articles/GSODR.html#appendices).  

Let's plot the a time-series of the daily minimum and maximum air temperatures from 2018-2021 at the selected met station.  


```{r}
met_df %>% 
  ggplot() +
  geom_line(aes(x = YEARMODA, y = MIN), color = "blue") +
  geom_line(aes(x = YEARMODA, y = MAX), color = "red") +
  
  labs(title = met_df$NAME[1],
       x = "Date",
       y = "Temperature (C)",
       caption = paste("Data source GSOD station", met_df$STNID[1])
       ) +
  
  theme_classic()
```

<br/> 

Oftentimes you have a study location/area of interest and you would like to identify any/all meterological stations that exist nearby your site.  The `GSODR` package has a really helpful function, `nearest_stations()` that does this.  You need to supply the lat/long of the area to query and the radial distance (in kilometers) from your search point.  The function will then return a list of the STNID with the station info for all stations that fall within your search radius.  

Let's test this out by searching for all of the stations within 25 km of Schenectady.  

```{r}
schdy_met_stations <- nearest_stations(LAT = 42.81,
                                  LON = -73.94,
                                  distance = 25)


schdy_met_stations
```

We can see that there are `r length(schdy_met_stations)` meteorological stations within 25 km of Schenectady.  

+ Download the data from 2010-2015 for the stations nearby Schenectady  
+ Try downloading meteorological data for a location you are interested in.  Create some figures and/or tables that provide insight into that locations meteorological conditions.  






## Hydrological data using the `dataRetrieval` package

The USGS [National Water Information Service (NWIS)](https://waterdata.usgs.gov/nwis) is an incredible database containing physical and chemical data on the streams/rivers, lakes, and groundwater for nearly 2 million locations across the US and US territories.  From NWIS you can download data related to thousands of different parameters including streamflow, groundwater levels, and a wide range of water chemistry and physical conditions.  

This data can be queried and retrieved using the `dataRetrieval` package that was developed and is maintained by the USGS.  Given the richness and volume of the data there are many functions and features within this package and some knowledge of how the available data is particularly helpful when using `dataRetrieval`.  Today we'll learn how to download daily streamflow data and also learn the basics of downloading water quality data.  For those interested in learning more I am helping to provide additional guidance and you can also explore the excellent [background](http://usgs-r.github.io/dataRetrieval/articles/dataRetrieval.html) and [tutorial](http://usgs-r.github.io/dataRetrieval/articles/tutorial.html) articles that have been put together by the developers of the package.  


When querying the NWIS database to find and/or download available data you generally need to specify a parameter (i.e., what was being measured) and a location/region of interest.  

If you already know the USGS site number (the unique numeric identifier the USGS assigns to sampling sites) and you know that the parameter of interest was measured at the site you can proceed to the data acquisition step.  However, many times we don't know what/if available sites with the desired data exist and we need to first identify those sites.  Once you've identified the sites with the desired data, you can then directly acquire the data for the site(s) of interest.  

Let's work through an example where we locate sites in Schenectady county that have mean daily streamflow and/or stream temperature data.  Note that we could look for other parameters (e.g., turbidity, concentration of chloride, pH,...) if we were interested in those measurements.  



### Identify sites with desired data  

```{r}
library(dataRetrieval)
```

https://waterservices.usgs.gov/rest/Site-Service.html

We are going to query the NWIS database for sites within Schenectady county (`countyCd = "36093"`) that have streamflow (00060) and/or gauge height (i.e., water level) (00065) data.  

Note that every US county has a FIPS county code, which is a unique numeric code that the US has assigned to each county.  You can find a list of all the [county codes here](https://help.waterdata.usgs.gov/code/county_query?fmt=html).  

You can also find a list of all of the [parameter codes used by the USGS here](https://help.waterdata.usgs.gov/codes-and-parameters/parameters).  
```{r}
sites_schdy <- whatNWISsites(countyCd = "36093", 
                             parameterCd = c("00060","00065"),
                             service = "dv"
                             )


sites_schdy
```

You can see that there are `r length(sites_schdy)` sites that meet the search criteria.  You'll also note that under the `site_tp_cd` column you see both *GW* and *ST*, which stand for groundwater and stream respectively.  Let's only keep the stream sites.


```{r}
sites_schdy <- sites_schdy %>% 
  filter(site_tp_cd == "ST")
```


Now let's get information about the available data (e.g., the number of observations, the length of the data record)

```{r}
sites_what_data <- whatNWISdata(siteNumber = sites_schdy$site_no, 
                            service = "dv", 
                            parameterCd = c("00060","00065"),
                            statCd = "00003")

sites_what_data
```

We can see that we have information about the data records for the sites with available water temperature (parm_cd = 00010) and streamflow (parm_cd = 00060).  Below is a map of these sites.  

```{r}
fig_map <- sites_what_data %>% 
  leaflet() %>% 
  addProviderTiles(providers$OpenStreetMap) %>% 
  addMarkers(~ dec_long_va, ~ dec_lat_va, popup = ~ station_nm)
```


Based on this information you might choose to acquire the data for some or all of these sites.  Since there are only a few sites, let's go ahead and download the water temperature and streamflow data for all of these sites.  

```{r}
df_stream_data <- readNWISdv(siteNumbers = sites_what_data$site_no,
                             parameterCd = c("00060","00065"),
                             statCd = "00003")
                               
```

When you download the data it will have column names that refer to parameter codes and other data descriptors that are not particularly human readable.  Let's use the `renameNWISColumns()` function, which renames the columns with more human readable names.  




```{r}
df_stream_data <- df_stream_data %>%  
  renameNWISColumns()
```



```{r fig.width= 9, fig.height= 4}
df_stream_data %>% 
  filter(!is.na(GH)) %>% 
  
  ggplot(aes(x =Date, y = GH)) +
  geom_line() +
  
  facet_wrap(~ site_no, scales = "free")
```

```{r fig.width= 9, fig.height= 4}

df_stream_data %>% 
  filter(!is.na(Flow)) %>% 
  
  ggplot(aes(x =Date, y = Flow)) +
  geom_line() +
  
  facet_wrap(~ site_no, scales = "free")
```



```{r}
siteINFO <- readNWISsite(sites_what_data$site_no)
```




```{r}
comment(siteINFO)
```


### water quality example

00915 (Calcium, filtered, mg/L)

00618
```{r}
pCode <- c("00618")
NY_NO3 <- whatNWISdata(stateCd="NY", 
                       parameterCd=pCode, startDate = "2015-01-01"
                     ) %>% 
  filter(site_tp_cd == "GW")

```



```{r}
NY_NO3_recent <- NY_NO3 %>% 
  filter(end_date > make_date(2020,01,01))
```


```{r}
NY_NO3_recent_data <- readNWISqw(siteNumbers = NY_NO3$site_no, 
                                 startDate = "2015-01-01", 
                                 parameterCd = pCode)
```


```{r}
NY_NO3_recent_data_summary <- NY_NO3_recent_data %>% 
  group_by(site_no) %>% 
  summarize(NO3_max = max(result_va, na.rm = T)) %>% 
  
  left_join(NY_NO3_recent)
```

```{r}

tmap_mode("view")

fig_map_NO3 <- NY_NO3_recent_data_summary %>% 
  filter(!is.na(dec_lat_va), !is.na(dec_long_va)) %>% 
  st_as_sf(coords = c("dec_long_va", "dec_lat_va")) %>% 
  
  tm_shape() +
  tm_dots(col = "NO3_max", style = "quantile", size = 0.2, palette = "viridis")
   
  
```



## rnoaa




## stationaRy



## elevatr


## rWBclimate
https://docs.ropensci.org/rWBclimate/


## ebird
https://cran.r-project.org/web/packages/rebird/vignettes/rebird_vignette.html
```{r}
ebird_key <- "c2jlhls8h84c"
```


```{r}
library(rebird)
```


```{r}
species_code('Accipiter cooperii')

```
```{r}
new_tax <- ebirdtaxonomy()
```

	
Sialia sialis
Haliaeetus leucocephalus
Accipiter cooperii
```{r}
a <- ebirdgeo(species = species_code('Accipiter cooperii'), lat = 42.8, lng = -73.9, back = 30, dist = 50,
              key = ebird_key)
```

```{r}
b <- ebirdgeo(lat = 42.8, lng = -73.9, back = 30,
              key = ebird_key)
```


```{r}
fig_map <- a %>% 
  leaflet() %>% 
  addProviderTiles(providers$Esri.NatGeoWorldMap) %>% 
  addMarkers(~ lng, ~ lat, popup = ~ paste(comName, "; n:", howMany))
```






